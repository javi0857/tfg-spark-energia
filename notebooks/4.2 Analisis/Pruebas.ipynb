{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c08163be-82e8-4570-891a-967a75099702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Loading <code>spark-stubs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Creating SparkSession\r\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://JAVITOP.mshome.net:4041\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
       "\n",
       "\n",
       "//Iniciamos session de Spark\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@50226ec8\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3`\n",
    "\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "\n",
    "//Reducir numero logs\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "\n",
    "//For adding extra dependenies\n",
    "import $ivy.`org.typelevel::cats-core:1.6.0`\n",
    "\n",
    "//Plotly\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.3`\n",
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "\n",
    "//Iniciamos session de Spark\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "     .master(\"local[*]\")\n",
    "    //.config(\"spark.executor.instances\", \"4\") // NÃºmero de ejecutores\n",
    "    //.config(\"spark.executor.memory\", \"4g\") // Memoria por ejecutor\n",
    "    //.config(\"spark.executor.cores\", \"2\") // NÃºcleos por ejecutor\n",
    "    //.config(\"spark.driver.memory\", \"4g\") // Memoria del driver\n",
    "    //.config(\"spark.sql.shuffle.partitions\", \"8\") // NÃºmero de particiones para operaciones de shuffle\n",
    "    .getOrCreate()\n",
    "}\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc714d1-79ac-469d-b925-234f7a0a1d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "| id|      name|price|\n",
      "+---+----------+-----+\n",
      "|  0|    Tomato|  2.0|\n",
      "|  1|Watermelon|  5.5|\n",
      "|  2| Pineapple|  7.0|\n",
      "+---+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| id|itemid|count|\n",
      "+---+------+-----+\n",
      "|100|     0|    1|\n",
      "|100|     1|    1|\n",
      "|101|     2|    3|\n",
      "|102|     2|    8|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\u001b[39m\r\n",
       "\u001b[36mitemsSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"id\"\u001b[39m, IntegerType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"name\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"price\"\u001b[39m, FloatType, \u001b[32mfalse\u001b[39m, {})\n",
       ")\r\n",
       "\u001b[36mordersSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"id\"\u001b[39m, IntegerType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"itemid\"\u001b[39m, IntegerType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"count\"\u001b[39m, IntegerType, \u001b[32mfalse\u001b[39m, {})\n",
       ")\r\n",
       "\u001b[36mitemsData\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  [0,Tomato,2.0],\n",
       "  [1,Watermelon,5.5],\n",
       "  [2,Pineapple,7.0]\n",
       ")\r\n",
       "\u001b[36mordersData\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mList\u001b[39m([100,0,1], [100,1,1], [101,2,3], [102,2,8])\r\n",
       "\u001b[36mitemsDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, name: string ... 1 more field]\r\n",
       "\u001b[36mordersDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, itemid: int ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "val itemsSchema = StructType(Array(\n",
    "    StructField(\"id\", IntegerType, false),\n",
    "    StructField(\"name\", StringType, false),\n",
    "    StructField(\"price\", FloatType, false)\n",
    "))\n",
    "\n",
    "val ordersSchema = StructType(Array(\n",
    "    StructField(\"id\", IntegerType, false),\n",
    "    StructField(\"itemid\", IntegerType, false),\n",
    "    StructField(\"count\", IntegerType, false)\n",
    "))\n",
    "\n",
    "// ðŸ“Œ Crear listas de datos\n",
    "val itemsData = Seq(\n",
    "    Row(0, \"Tomato\", 2.0f),\n",
    "    Row(1, \"Watermelon\", 5.5f),\n",
    "    Row(2, \"Pineapple\", 7.0f)\n",
    ")\n",
    "\n",
    "val ordersData = Seq(\n",
    "    Row(100, 0, 1),\n",
    "    Row(100, 1, 1),\n",
    "    Row(101, 2, 3),\n",
    "    Row(102, 2, 8)\n",
    ")\n",
    "\n",
    "// ðŸ“Œ Crear DataFrames\n",
    "val itemsDF = spark.createDataFrame(spark.sparkContext.parallelize(itemsData), itemsSchema)\n",
    "val ordersDF = spark.createDataFrame(spark.sparkContext.parallelize(ordersData), ordersSchema)\n",
    "\n",
    "// ðŸ“Œ Mostrar los DataFrames\n",
    "itemsDF.show()\n",
    "ordersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc62d16-c2c3-4cf4-a6af-609ad8e0879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---+\n",
      "|     name|price|  c|\n",
      "+---------+-----+---+\n",
      "|Pineapple|  7.0| 11|\n",
      "+---------+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\r\n",
       "\u001b[36my\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, price: float ... 1 more field]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val y= itemsDF\n",
    "    .join(ordersDF, itemsDF(\"id\") === ordersDF(\"itemid\"), \"inner\")\n",
    "    .where(itemsDF(\"id\") === 2)\n",
    "    .groupBy(\"name\",\"price\")\n",
    "    .agg(sum(\"count\").alias(\"c\")) \n",
    "\n",
    "\n",
    "y.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f12e6f3f-51af-4bb6-9d63-ba9985e007d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[name#4, price#5], functions=[sum(cast(count#14 as bigint))])\n",
      "+- Exchange hashpartitioning(name#4, price#5, 200)\n",
      "   +- *(5) HashAggregate(keys=[name#4, price#5], functions=[partial_sum(cast(count#14 as bigint))])\n",
      "      +- *(5) Project [name#4, price#5, count#14]\n",
      "         +- *(5) SortMergeJoin [id#3], [itemid#13], Inner\n",
      "            :- *(2) Sort [id#3 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(id#3, 200)\n",
      "            :     +- *(1) Filter (id#3 = 2)\n",
      "            :        +- Scan ExistingRDD[id#3,name#4,price#5]\n",
      "            +- *(4) Sort [itemid#13 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(itemid#13, 200)\n",
      "                  +- *(3) Project [itemid#13, count#14]\n",
      "                     +- *(3) Filter (itemid#13 = 2)\n",
      "                        +- Scan ExistingRDD[id#12,itemid#13,count#14]\n"
     ]
    }
   ],
   "source": [
    "y.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee4d04-0952-4f3a-9b4c-b3210aa8aa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
